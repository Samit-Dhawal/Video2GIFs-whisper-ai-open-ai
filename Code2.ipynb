{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (from moviepy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (from moviepy) (2.32.3)\n",
      "Requirement already satisfied: proglog<=1.0.0 in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (from moviepy) (2.0.0)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (from moviepy) (2.34.1)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (from moviepy) (0.5.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy) (10.3.0)\n",
      "Requirement already satisfied: setuptools in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (from imageio_ffmpeg>=0.2.0->moviepy) (63.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (2024.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n",
      "Requirement already satisfied: colorama in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.0-cp310-cp310-win_amd64.whl (11.0 MB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in d:\\personalprojects\\videotogif\\venv\\lib\\site-packages (from scikit-learn) (2.0.0)\n",
      "Collecting scipy>=1.6.0\n",
      "  Using cached scipy-1.13.1-cp310-cp310-win_amd64.whl (46.2 MB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.0 scipy-1.13.1 threadpoolctl-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install imageio-ffmpeg\n",
    "!pip install moviepy\n",
    "!pip install scikit-learn\n",
    "!pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import subprocess\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Audio from Video for Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in ./Output/CaptionsGenerate.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio extracted successfully: ./Output/CaptionsGenerate.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def extract_audio_with_moviepy(video_path, output_audio_path):\n",
    "    try:\n",
    "        video = VideoFileClip(video_path)\n",
    "        video.audio.write_audiofile(output_audio_path)\n",
    "        print(f'Audio extracted successfully: {output_audio_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error extracting audio: {e}')\n",
    "\n",
    "# Define paths\n",
    "video_file_path = './CaptionsGenerate.mp4'  # Replace with your video file path\n",
    "audio_file_path = './Output/CaptionsGenerate.mp3'  # Extracted audio file path\n",
    "\n",
    "# Extract audio from video\n",
    "extract_audio_with_moviepy(video_file_path, audio_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transcribing Audio to Text with Whisper API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file: ./Output/CaptionsGenerate.mp3\n",
      "Saved translation to ./Output\\CaptionsGenerateSeg.json\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Define the output directory for JSON and GIFs\n",
    "OUTPUT_DIR = './Output'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Function to transcribe audio using Whisper\n",
    "def transcribe_audio_with_whisper(audio_path):\n",
    "    print(f'Processing audio file: {audio_path}')\n",
    "    with open(audio_path, 'rb') as audio_file:\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            file=audio_file,\n",
    "            model='whisper-1',\n",
    "            response_format='verbose_json',\n",
    "            timestamp_granularities=['word']\n",
    "        )\n",
    "        \n",
    "\n",
    "    transcription_data = {                #word level\n",
    "    \"text\": transcription.text,\n",
    "    \"task\": transcription.task,\n",
    "    \"language\": transcription.language,\n",
    "    \"duration\": transcription.duration,\n",
    "    \"words\": transcription.words\n",
    "    }\n",
    "\n",
    "    # Define the output filename\n",
    "    output_filename = os.path.splitext(os.path.basename(audio_file_path))[0] + '.json'\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "    # Saving the translated text with timestamps to a JSON file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(transcription_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # Print the path where the JSON file is saved\n",
    "    print(f'Saved translation to {output_path}')\n",
    "\n",
    "# Transcribe the extracted audio file\n",
    "transcription_file_path = transcribe_audio_with_whisper(audio_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Expressions from the transcribed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the test directly on this video. I'm gonna make some gifts. I love you. Grand Rising. Good vibes to all. Stay based. Thumbs up. Thumbs down. I'm okay. Happy Monday. Happy Tuesday. Happy Wednesday. Happy Thursday. Happy Friday. And a good New Year's Eve to everyone. And so basically it should choose where to section off each gift. It should be pretty obvious.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Output/CaptionsGenerate.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "transcription_text = data['text']\n",
    "transcription_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an expert in analyzing and extracting notable moments and emotional expressions from transcriptions of video content. Your task is to meticulously analyze the provided transcription text to identify segments that capture distinct emotions, reactions, and significant expressions suitable for creating GIFs. You will output the results in a strict JSON format following a specified structure.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f'''\n",
    "        Identify and extract segments from the following transcription text that are suitable for creating GIFs. Focus on moments that display clear emotions, reactions, and notable expressions, such as:\n",
    "\n",
    "        - **Greetings and Farewells**: Friendly or enthusiastic opening and closing phrases like \"Hello everyone\", \"Goodbye\", \"Grand Rising\", \"Happy Monday\", etc.\n",
    "        - **Sudden Emotions**: Exclamatory expressions or short, impactful phrases like \"Wow!\", \"Oh no!\", \"Amazing!\", \"I love you\", etc.\n",
    "        - **Emotional Statements**: Sentences that convey a strong feeling like \"I'm so happy\", \"This is frustrating\", \"I'm okay\", etc.\n",
    "        - **Reactions to Events**: Phrases showing a response to something happening like \"That's incredible\", \"I can't believe it\", etc.\n",
    "        - **Laughter and Joy**: Any instances of laughter or expressions of joy like \"Hahaha\", \"That's hilarious\", \"Woohoo!\", etc.\n",
    "        - **Excitement**: Enthusiastic or eager statements like \"This is awesome\", \"I'm so excited\", \"Can't wait for this\", etc.\n",
    "        - **Surprises and Shock**: Reactions to unexpected events like \"No way!\", \"What a surprise\", etc.\n",
    "        - **Emphatic Statements**: Strongly asserted phrases like \"Absolutely!\", \"Definitely!\", \"Without a doubt\", etc.\n",
    "        - **Questioning or Curiosity**: Inquisitive expressions like \"What's happening?\", \"Why is that?\", \"How does this work?\", etc.\n",
    "        - **Expressive Pauses**: Short, impactful sounds or words that convey a pause or hesitation like \"Uh-oh\", \"Hmm\", \"Ahh\", etc.\n",
    "        - **Celebrations and Wishes**: Phrases used to celebrate or send good wishes like \"Cheers!\", \"Good vibes to all\", \"Stay based\", \"And a good New Year's Eve to everyone\", etc.\n",
    "        - **Simple Gestures**: Indications of physical gestures like \"Thumbs up\", \"Thumbs down\".\n",
    "\n",
    "        These moments are characterized by distinct phrasing, changes in tone, emotional emphasis, or visual cues that make them suitable for creating engaging and expressive GIFs.\n",
    "\n",
    "        Below is the transcription text to analyze:\n",
    "\n",
    "        \"{transcription_text}\"\n",
    "\n",
    "        Return the identified segments in JSON format, including the type of emotion or expression and the text of each identified segment. Do not alter any text from the provided transcription.\n",
    "\n",
    "        The output should strictly follow this format:\n",
    "        {{\n",
    "            \"segments\": [\n",
    "                {{\n",
    "                    \"type\": \"Emotion or expression type\",\n",
    "                    \"text\": \"Segment text\"\n",
    "                }},\n",
    "                ...\n",
    "            ]\n",
    "        }}\n",
    "\n",
    "        Ensure that:\n",
    "        1. Each segment captures a distinct emotion or expression suitable for a GIF.\n",
    "        2. The output adheres to the specified JSON format without any deviations.\n",
    "        3. No text is changed from the original transcription.\n",
    "        4. The type of emotion or expression is accurately identified or reasonably inferred based on the segment context.\n",
    "        '''\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exp_segments(prompt, model):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages= prompt,\n",
    "        response_format={\"type\":\"json_object\"},\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gpt-3.5-turbo'\n",
    "segments = extract_exp_segments(messages, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segments': [{'type': 'Sudden Emotions', 'text': 'I love you.'},\n",
       "  {'type': 'Greetings and Farewells', 'text': 'Grand Rising.'},\n",
       "  {'type': 'Celebrations and Wishes', 'text': 'Good vibes to all.'},\n",
       "  {'type': 'Celebrations and Wishes', 'text': 'Stay based.'},\n",
       "  {'type': 'Simple Gestures', 'text': 'Thumbs up.'},\n",
       "  {'type': 'Simple Gestures', 'text': 'Thumbs down.'},\n",
       "  {'type': 'Emotional Statements', 'text': \"I'm okay.\"},\n",
       "  {'type': 'Greetings and Farewells', 'text': 'Happy Monday.'},\n",
       "  {'type': 'Greetings and Farewells', 'text': 'Happy Tuesday.'},\n",
       "  {'type': 'Greetings and Farewells', 'text': 'Happy Wednesday.'},\n",
       "  {'type': 'Greetings and Farewells', 'text': 'Happy Thursday.'},\n",
       "  {'type': 'Greetings and Farewells', 'text': 'Happy Friday.'},\n",
       "  {'type': 'Celebrations and Wishes',\n",
       "   'text': \"And a good New Year's Eve to everyone.\"}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = segments.choices[0].message.content\n",
    "jso = json.loads(res)\n",
    "jso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text: remove punctuation and lowercase\n",
    "def preprocess_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Tokenize text\n",
    "def tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting timestamps of the expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract timestamps for the expressions texts\n",
    "def extract_timestamps(transcription, words, exp_data):\n",
    "    text = preprocess_text(transcription)\n",
    "    words_clean = [{'word': preprocess_text(w['word']), 'start': w['start'], 'end': w['end']} for w in words]\n",
    "    transcription_tokens = [w['word'] for w in words_clean]\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer().fit(transcription_tokens)\n",
    "\n",
    "    for exp in exp_data['segments']:\n",
    "        advertisement_text = preprocess_text(exp['text'])\n",
    "        advertisement_tokens = tokenize(advertisement_text)\n",
    "        \n",
    "        # Transform the expressions and transcription chunks into TF-IDF vectors\n",
    "        ad_vector = vectorizer.transform([\" \".join(advertisement_tokens)])\n",
    "        transcription_vectors = vectorizer.transform([\" \".join(transcription_tokens[i:i+len(advertisement_tokens)]) for i in range(len(transcription_tokens) - len(advertisement_tokens) + 1)])\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        cosine_similarities = cosine_similarity(ad_vector, transcription_vectors).flatten()\n",
    "        \n",
    "        # Find the best match index\n",
    "        best_match_idx = np.argmax(cosine_similarities)\n",
    "        \n",
    "        # Get the start and end timestamps\n",
    "        if best_match_idx != -1:\n",
    "            start_timestamp = words_clean[best_match_idx]['start']\n",
    "            end_timestamp = words_clean[best_match_idx + len(advertisement_tokens) - 1]['end']\n",
    "            \n",
    "            results.append({\n",
    "                'type': exp['type'],\n",
    "                'exp_text': exp['text'],\n",
    "                'start_timestamp': start_timestamp,\n",
    "                'end_timestamp': end_timestamp\n",
    "            })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"type\": \"Sudden Emotions\",\n",
      "  \"exp_text\": \"I love you.\",\n",
      "  \"start_timestamp\": 5.78000020980835,\n",
      "  \"end_timestamp\": 9.0600004196167\n",
      "}\n",
      "{\n",
      "  \"type\": \"Greetings and Farewells\",\n",
      "  \"exp_text\": \"Grand Rising.\",\n",
      "  \"start_timestamp\": 11.899999618530273,\n",
      "  \"end_timestamp\": 13.319999694824219\n",
      "}\n",
      "{\n",
      "  \"type\": \"Celebrations and Wishes\",\n",
      "  \"exp_text\": \"Good vibes to all.\",\n",
      "  \"start_timestamp\": 15.779999732971191,\n",
      "  \"end_timestamp\": 16.8799991607666\n",
      "}\n",
      "{\n",
      "  \"type\": \"Celebrations and Wishes\",\n",
      "  \"exp_text\": \"Stay based.\",\n",
      "  \"start_timestamp\": 18.959999084472656,\n",
      "  \"end_timestamp\": 19.68000030517578\n",
      "}\n",
      "{\n",
      "  \"type\": \"Simple Gestures\",\n",
      "  \"exp_text\": \"Thumbs up.\",\n",
      "  \"start_timestamp\": 22.360000610351562,\n",
      "  \"end_timestamp\": 23.020000457763672\n",
      "}\n",
      "{\n",
      "  \"type\": \"Simple Gestures\",\n",
      "  \"exp_text\": \"Thumbs down.\",\n",
      "  \"start_timestamp\": 24.719999313354492,\n",
      "  \"end_timestamp\": 25.18000030517578\n",
      "}\n",
      "{\n",
      "  \"type\": \"Emotional Statements\",\n",
      "  \"exp_text\": \"I'm okay.\",\n",
      "  \"start_timestamp\": 27.399999618530273,\n",
      "  \"end_timestamp\": 29.34000015258789\n",
      "}\n",
      "{\n",
      "  \"type\": \"Greetings and Farewells\",\n",
      "  \"exp_text\": \"Happy Monday.\",\n",
      "  \"start_timestamp\": 34.959999084472656,\n",
      "  \"end_timestamp\": 36.65999984741211\n",
      "}\n",
      "{\n",
      "  \"type\": \"Greetings and Farewells\",\n",
      "  \"exp_text\": \"Happy Tuesday.\",\n",
      "  \"start_timestamp\": 38.34000015258789,\n",
      "  \"end_timestamp\": 39.099998474121094\n",
      "}\n",
      "{\n",
      "  \"type\": \"Greetings and Farewells\",\n",
      "  \"exp_text\": \"Happy Wednesday.\",\n",
      "  \"start_timestamp\": 40.380001068115234,\n",
      "  \"end_timestamp\": 41.0\n",
      "}\n",
      "{\n",
      "  \"type\": \"Greetings and Farewells\",\n",
      "  \"exp_text\": \"Happy Thursday.\",\n",
      "  \"start_timestamp\": 42.279998779296875,\n",
      "  \"end_timestamp\": 43.099998474121094\n",
      "}\n",
      "{\n",
      "  \"type\": \"Greetings and Farewells\",\n",
      "  \"exp_text\": \"Happy Friday.\",\n",
      "  \"start_timestamp\": 44.400001525878906,\n",
      "  \"end_timestamp\": 44.97999954223633\n",
      "}\n",
      "{\n",
      "  \"type\": \"Celebrations and Wishes\",\n",
      "  \"exp_text\": \"And a good New Year's Eve to everyone.\",\n",
      "  \"start_timestamp\": 45.79999923706055,\n",
      "  \"end_timestamp\": 49.29999923706055\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "transcription_text = data['text']\n",
    "word_list = data['words']\n",
    "exp_segments_with_timestamps = extract_timestamps(transcription_text, word_list, jso)\n",
    "\n",
    "# Print the results\n",
    "for exp in exp_segments_with_timestamps:\n",
    "    print(json.dumps(exp, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ./Output/CaptionOutput.json\n"
     ]
    }
   ],
   "source": [
    "# Save the results to a JSON file\n",
    "output_file = \"./Output/CaptionOutput.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(exp_segments_with_timestamps, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Video to GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 2024-06-16-git-fcf72966a5-full_build-www.gyan.dev Copyright (c) 2000-2024 the FFmpeg developers\n",
      "built with gcc 13.2.0 (Rev5, Built by MSYS2 project)\n",
      "configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-bzlib --enable-lzma --enable-libsnappy --enable-zlib --enable-librist --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-libbluray --enable-libcaca --enable-sdl2 --enable-libaribb24 --enable-libaribcaption --enable-libdav1d --enable-libdavs2 --enable-libuavs3d --enable-libxevd --enable-libzvbi --enable-librav1e --enable-libsvtav1 --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxavs2 --enable-libxeve --enable-libxvid --enable-libaom --enable-libjxl --enable-libopenjpeg --enable-libvpx --enable-mediafoundation --enable-libass --enable-frei0r --enable-libfreetype --enable-libfribidi --enable-libharfbuzz --enable-liblensfun --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-dxva2 --enable-d3d11va --enable-d3d12va --enable-ffnvcodec --enable-libvpl --enable-nvdec --enable-nvenc --enable-vaapi --enable-libshaderc --enable-vulkan --enable-libplacebo --enable-opencl --enable-libcdio --enable-libgme --enable-libmodplug --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libshine --enable-libtheora --enable-libtwolame --enable-libvo-amrwbenc --enable-libcodec2 --enable-libilbc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-ladspa --enable-libbs2b --enable-libflite --enable-libmysofa --enable-librubberband --enable-libsoxr --enable-chromaprint\n",
      "libavutil      59. 22.100 / 59. 22.100\n",
      "libavcodec     61.  8.100 / 61.  8.100\n",
      "libavformat    61.  3.104 / 61.  3.104\n",
      "libavdevice    61.  2.100 / 61.  2.100\n",
      "libavfilter    10.  2.102 / 10.  2.102\n",
      "libswscale      8.  2.100 /  8.  2.100\n",
      "libswresample   5.  2.100 /  5.  2.100\n",
      "libpostproc    58.  2.100 / 58.  2.100\n"
     ]
    }
   ],
   "source": [
    "!ffmpeg -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF created with caption 'I love you.' and saved as GIF_Created\\I_love_you..gif\n",
      "GIF created with caption 'Grand Rising.' and saved as GIF_Created\\Grand_Rising..gif\n",
      "GIF created with caption 'Good vibes to all.' and saved as GIF_Created\\Good_vibes_to_all..gif\n",
      "GIF created with caption 'Stay based.' and saved as GIF_Created\\Stay_based..gif\n",
      "GIF created with caption 'Thumbs up.' and saved as GIF_Created\\Thumbs_up..gif\n",
      "GIF created with caption 'Thumbs down.' and saved as GIF_Created\\Thumbs_down..gif\n",
      "GIF created with caption 'I'm okay.' and saved as GIF_Created\\Im_okay..gif\n",
      "GIF created with caption 'Happy Monday.' and saved as GIF_Created\\Happy_Monday..gif\n",
      "GIF created with caption 'Happy Tuesday.' and saved as GIF_Created\\Happy_Tuesday..gif\n",
      "GIF created with caption 'Happy Wednesday.' and saved as GIF_Created\\Happy_Wednesday..gif\n",
      "GIF created with caption 'Happy Thursday.' and saved as GIF_Created\\Happy_Thursday..gif\n",
      "GIF created with caption 'Happy Friday.' and saved as GIF_Created\\Happy_Friday..gif\n",
      "GIF created with caption 'And a good New Year's Eve to everyone.' and saved as GIF_Created\\And_a_good_New_Years_Eve_to_everyone..gif\n",
      "All videos converted to GIFs with captions successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import ffmpeg\n",
    "\n",
    "# Define paths\n",
    "json_file_path = os.path.join(\"Output\", \"CaptionOutput.json\")\n",
    "video_file_path = \"CaptionsGenerate.mp4\"\n",
    "output_dir = \"GIF_Output\"\n",
    "gif_output_dir = \"GIF_Created\"\n",
    "font_file_path = \"Arial Bold.ttf\"  # Full path to your font file\n",
    "\n",
    "# Ensure the output directories exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if not os.path.exists(gif_output_dir):\n",
    "    os.makedirs(gif_output_dir)\n",
    "\n",
    "# Read segment data from JSON file\n",
    "with open(json_file_path, 'r') as file:\n",
    "    segments = json.load(file)\n",
    "\n",
    "# Function to trim video segments based on timestamps\n",
    "def trim_video_segment(input_video, output_video, start_time, end_time):\n",
    "    (\n",
    "        ffmpeg\n",
    "        .input(input_video, ss=start_time, to=end_time)\n",
    "        .output(output_video, c=\"copy\")\n",
    "        .overwrite_output()\n",
    "        .run(capture_stderr=True)\n",
    "    )\n",
    "\n",
    "# Function to overlay text on video and convert to GIF\n",
    "def create_gif_with_text(segment, output_dir, gif_output_dir, font_file_path):\n",
    "    start_time = segment[\"start_timestamp\"]\n",
    "    end_time = segment[\"end_timestamp\"]\n",
    "    text = segment[\"exp_text\"]\n",
    "    \n",
    "    # Output file name and paths\n",
    "    safe_text = text.replace(' ', '_').replace('\\'', '')\n",
    "    video_output_filename = f\"{safe_text}.mp4\"\n",
    "    video_output_path = os.path.join(output_dir, video_output_filename)\n",
    "    gif_output_filename = f\"{safe_text}.gif\"\n",
    "    gif_output_path = os.path.join(gif_output_dir, gif_output_filename)\n",
    "    \n",
    "    # Trim the video segment\n",
    "    trim_video_segment(video_file_path, video_output_path, start_time, end_time)\n",
    "    \n",
    "    # Overlay text on video and convert to GIF\n",
    "    (\n",
    "        ffmpeg\n",
    "        .input(video_output_path)\n",
    "        .output(gif_output_path, vf=f\"drawtext=text='{text}':fontsize=100:fontcolor=red:fontfile={font_file_path}:x=(w-text_w)/2:y=h-th-10:bordercolor=white:borderw=7\", vframes=25)\n",
    "        .overwrite_output()\n",
    "        .run(capture_stderr=True)\n",
    "    )\n",
    "    \n",
    "    print(f\"GIF created with caption '{text}' and saved as {gif_output_path}\")\n",
    "\n",
    "# Process each segment and create GIFs\n",
    "for segment in segments:\n",
    "    create_gif_with_text(segment, output_dir, gif_output_dir, font_file_path)\n",
    "\n",
    "print(\"All videos converted to GIFs with captions successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        By Samit Dhawal \n",
    "        https://linkedin.com/in/samitdhawal/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
